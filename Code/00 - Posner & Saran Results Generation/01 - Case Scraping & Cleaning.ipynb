{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF YOU CHOOSE TO RUN THIS SCRIPT INDIVIDUALLY, REPLACE \"~/Desktop/Univ. of Chicago/AI Judges/\" WITH THE ACTUAL FILE PATH WHERE THE REPLICATION PACKAGE IS STORED ON YOUR COMPUTER\n",
    "user_file_path = os.path.expanduser(\"~/Desktop/Univ. of Chicago/AI Judges/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where HTML files are stored (gathered from Spamann & Klohn)\n",
    "html_directory = os.path.join(user_file_path , \"Replication Package\", \"Experiment Materials\", \"Spamann & Klohn Files\")\n",
    "\n",
    "# Directory where extracted texts will be stored\n",
    "output_directory = os.path.join(user_file_path, \"Replication Package\", \"Experiment Materials\", \"Input Materials\", \"Cases & Statute\")\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Mapping of case names to their corresponding HTML file names\n",
    "file_paths = {\n",
    "    \"Appeal_Sainovic\": \"sainovic.html\",\n",
    "    \"Appeal_Vasiljevic\": \"vasiljevic.html\",\n",
    "    \"Statute\": \"index.html\",\n",
    "    \"Trial_Horvat_1\": \"Croatian_part1.html\",\n",
    "    \"Trial_Horvat_2\": \"Croatian_part2.html\",\n",
    "    \"Trial_Horvat_3\": \"Croatian_part3.html\",\n",
    "    \"Trial_Vukovic_1\": \"part1.html\",\n",
    "    \"Trial_Vukovic_2\": \"part2.html\",\n",
    "    \"Trial_Vukovic_3\": \"part3.html\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text and format it\n",
    "def get_text_with_formatting(text):\n",
    "    # Format text into paragraphs to maintain consistency with actual documents\n",
    "    for element in text.find_all(\"br\"):\n",
    "        element.replace_with(\"\\n\")\n",
    "    \n",
    "    # Replace footnotes (<a class=\"fn\">) with random, unique holder (which will later be removed)\n",
    "    for fn_element in text.find_all(class_=\"fn\"):\n",
    "        fn_element.replace_with(\"__FN__HOLDER__\")\n",
    "\n",
    "    # Remove HTML comments\n",
    "    comments = text.find_all(string=lambda text: isinstance(text, Comment))\n",
    "    for comment in comments:\n",
    "        comment.extract()\n",
    "    \n",
    "    # Preserve structure of documents\n",
    "    parts = []\n",
    "    for element in text.descendants:\n",
    "        if isinstance(element, str):\n",
    "            parts.append(element.strip())\n",
    "        elif element.name in [\"p\", \"div\"]:\n",
    "            parts.append(\"\\n\\n\")  # Double new line for paragraphs\n",
    "        elif element.name in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]:\n",
    "            parts.append(\"\\n\")\n",
    "        elif element.name in [\"ul\", \"ol\"]:\n",
    "            parts.append(\"\\n\")\n",
    "        elif element.name == \"li\":\n",
    "            parts.append(\"- \" + \"\\n\")\n",
    "    \n",
    "    # Join text and strip leading/trailing whitespace\n",
    "    return \"\".join(parts).strip()\n",
    "\n",
    "# Initialize dictionaries to store extracted text parts for Horvat and Vukovic\n",
    "horvat_parts = []\n",
    "vukovic_parts = []\n",
    "\n",
    "# Loop through each file path and name\n",
    "for case_name, file_name in file_paths.items():\n",
    "    try:\n",
    "        # Open the HTML file\n",
    "        html_file_path = os.path.join(html_directory, file_name)\n",
    "        with open(html_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            html_content = file.read()\n",
    "\n",
    "        # Remove all {% ... %} blocks\n",
    "        cleaned_content = re.sub(re.compile(r'{%.*?%}', re.DOTALL), '', html_content)\n",
    "\n",
    "        # Parse the HTML content via BeautifulSoup\n",
    "        text = BeautifulSoup(cleaned_content, \"html.parser\")\n",
    "\n",
    "        # Extract the formatted text\n",
    "        extracted_text = get_text_with_formatting(text)\n",
    "        extracted_text = extracted_text.replace(\"__FN__HOLDER__\", \" \") # Remove footnotes for clarity\n",
    "        extracted_text = extracted_text.replace(\"UNITEDNATIONS\", \"UNITED NATIONS\") # Correcting small typo from Spamann/Klohn experiment\n",
    "        \n",
    "        # Store the extracted text in their respective lists\n",
    "        if \"Horvat\" in case_name:\n",
    "            horvat_parts.append(extracted_text)\n",
    "        elif \"Vukovic\" in case_name:\n",
    "            vukovic_parts.append(extracted_text)\n",
    "        else:\n",
    "            globals()[case_name] = extracted_text\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {html_file_path}\")\n",
    "        continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {html_file_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "globals()[\"Trial_Horvat\"] = \"\\n\\n\".join(horvat_parts)\n",
    "globals()[\"Trial_Vukovic\"] = \"\\n\\n\".join(vukovic_parts)\n",
    "\n",
    "# Calculate half the length of each text\n",
    "overlap_size = 500 \n",
    "horvat_length = len(Trial_Horvat) // 2\n",
    "vukovic_length = len(Trial_Vukovic) // 2\n",
    "\n",
    "# Split exactly in half\n",
    "Trial_Horvat_1 = Trial_Horvat[:horvat_length + overlap_size]\n",
    "Trial_Horvat_2 = Trial_Horvat[horvat_length - overlap_size:]\n",
    "\n",
    "Trial_Vukovic_1 = Trial_Vukovic[:vukovic_length + overlap_size]\n",
    "Trial_Vukovic_2 = Trial_Vukovic[vukovic_length - overlap_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write each text object to a separate .txt file\n",
    "texts = [\n",
    "    (\"Appeal_Sainovic.txt\", Appeal_Sainovic),\n",
    "    (\"Appeal_Vasiljevic.txt\", Appeal_Vasiljevic),\n",
    "    (\"Statute.txt\", Statute),\n",
    "    (\"Trial_Horvat_1.txt\", Trial_Horvat_1),\n",
    "    (\"Trial_Horvat_2.txt\", Trial_Horvat_2),\n",
    "    (\"Trial_Horvat.txt\", Trial_Horvat),\n",
    "    (\"Trial_Vukovic_1.txt\", Trial_Vukovic_1),\n",
    "    (\"Trial_Vukovic_2.txt\", Trial_Vukovic_2),\n",
    "    (\"Trial_Vukovic.txt\", Trial_Vukovic)\n",
    "]\n",
    "\n",
    "# Loop through the list and write each text object to a file\n",
    "for filename, text in texts:\n",
    "    with open(os.path.join(output_directory, filename), \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appeal_Sainovic Word count: 38509\n",
      "Appeal_Sainovic Token count: 58656\n",
      "Appeal_Vasiljevic Word count: 34825\n",
      "Appeal_Vasiljevic Token count: 48997\n",
      "Trial_Horvat Word count: 163937\n",
      "Trial_Horvat Token count: 227139\n",
      "Trial_Vukovic Word count: 164596\n",
      "Trial_Vukovic Token count: 230477\n",
      "Statute Word count: 5465\n",
      "Statute Token count: 7997\n",
      "Horvat Sainovic Token count: 230590\n",
      "Horvat Vasiljevic Token count: 220931\n",
      "Vukovic Sainovic Token count: 297130\n",
      "Vukovic Vasiljevic Token count: 287471\n"
     ]
    }
   ],
   "source": [
    "# ADDITIONAL CODE (OPTIONAL)\n",
    "# Calculate word & token counts\n",
    "import tiktoken\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "Appeal_Sainovic_WC = len(Appeal_Sainovic.split())\n",
    "Appeal_Sainovic_TC = num_tokens_from_string(Appeal_Sainovic, \"gpt-4o\")\n",
    "print(f\"Appeal_Sainovic Word count: {Appeal_Sainovic_WC}\")\n",
    "print(f\"Appeal_Sainovic Token count: {Appeal_Sainovic_TC}\")\n",
    "\n",
    "Appeal_Vasiljevic_WC = len(Appeal_Vasiljevic.split())\n",
    "Appeal_Vasiljevic_TC = num_tokens_from_string(Appeal_Vasiljevic, \"gpt-4o\")\n",
    "print(f\"Appeal_Vasiljevic Word count: {Appeal_Vasiljevic_WC}\")\n",
    "print(f\"Appeal_Vasiljevic Token count: {Appeal_Vasiljevic_TC}\")\n",
    "\n",
    "Trial_Horvat_WC = len(Trial_Horvat.split())\n",
    "Trial_Horvat_TC = num_tokens_from_string(Trial_Horvat, \"gpt-4o\")\n",
    "print(f\"Trial_Horvat Word count: {Trial_Horvat_WC}\")\n",
    "print(f\"Trial_Horvat Token count: {Trial_Horvat_TC}\")\n",
    "\n",
    "Trial_Vukovic_WC = len(Trial_Vukovic.split())\n",
    "Trial_Vukovic_TC = num_tokens_from_string(Trial_Vukovic, \"gpt-4o\")\n",
    "print(f\"Trial_Vukovic Word count: {Trial_Vukovic_WC}\")\n",
    "print(f\"Trial_Vukovic Token count: {Trial_Vukovic_TC}\")\n",
    "\n",
    "Statute_WC = len(Statute.split())\n",
    "Statute_TC = num_tokens_from_string(Statute, \"gpt-4o\")\n",
    "print(f\"Statute Word count: {Statute_WC}\")\n",
    "print(f\"Statute Token count: {Statute_TC}\")\n",
    "\n",
    "Total_TC_HS = Appeal_Sainovic_TC + Trial_Horvat_WC + Statute_TC\n",
    "Total_TC_HV = Appeal_Vasiljevic_TC + Trial_Horvat_WC + Statute_TC\n",
    "Total_TC_VS = Appeal_Sainovic_TC + Trial_Vukovic_TC + Statute_TC\n",
    "Total_TC_VV = Appeal_Vasiljevic_TC + Trial_Vukovic_TC + Statute_TC\n",
    "\n",
    "print(f\"Horvat Sainovic Token count: {Total_TC_HS}\")\n",
    "print(f\"Horvat Vasiljevic Token count: {Total_TC_HV}\")\n",
    "print(f\"Vukovic Sainovic Token count: {Total_TC_VS}\")\n",
    "print(f\"Vukovic Vasiljevic Token count: {Total_TC_VV}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
